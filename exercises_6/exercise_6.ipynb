{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Setting Up and Running a VLM\n",
    "\n",
    "This notebook demonstrates how to install dependencies, download a model from Hugging Face,\n",
    "and load it using the `llama_cpp` library. Follow the steps below to understand how to:\n",
    "- Install necessary Python packages.\n",
    "- Download a GGUF model file from Hugging Face.\n",
    "- Load and interact with the model.\n",
    "\n",
    "Make sure you have the required dependencies installed before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary dependencies for running the Llama model\n",
    "!pip3 install llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124\n",
    "!pip3 install huggingface_hub==0.28.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for model execution and downloading\n",
    "from llama_cpp import Llama\n",
    "from llama_cpp.llama_chat_format import MoondreamChatHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the chat handler for the VLM\n",
    "chat_handler = MoondreamChatHandler.from_pretrained(\n",
    "    repo_id=\"vikhyatk/moondream2\",\n",
    "    filename=\"*mmproj*\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the VLM\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"vikhyatk/moondream2\",\n",
    "    filename=\"*text-model*\",\n",
    "    chat_handler=chat_handler,\n",
    "    n_ctx=4096,  # should be increased to accommodate the image embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt the VLM using Frieren image\n",
    "llm.create_chat_completion(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What is the girl of the image eating?\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\n",
    "                    \"url\": \"https://media.tenor.com/p78411-TVP4AAAAe/hamburger-frieren.png\"}}\n",
    "\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")[\"choices\"][0][\"message\"][\"content\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
