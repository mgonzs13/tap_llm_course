{"cells":[{"cell_type":"markdown","metadata":{"id":"9f3491ff"},"source":["# Exercise: Tool Calling\n","\n","This notebook guides you through setting up and running a basic AI Agent using `LangChain`.\n","The steps include:\n","\n","1. Installing required dependencies.\n","2. Importing necessary libraries.\n","3. Setting up the language model and the AI Agent.\n","4. Prompting the AI Agent.\n","\n","Follow the instructions in the code cells and ensure all dependencies are installed correctly before proceeding.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Hy4hPCzvgqh"},"outputs":[],"source":["# Check GPU availability\n","!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2UcTgkNEahwC"},"outputs":[],"source":["# Install required dependencies\n","!pip3 install llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124\n","!pip3 install huggingface_hub==0.28.0\n","!pip3 install langchain==0.3.17 langchain-core==0.3.33 langchain-community==0.3.14"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XOrRc1SBavJg"},"outputs":[],"source":["# Import necessary libraries for downloading models and setting up the chat system\n","from random import randint\n","from huggingface_hub import hf_hub_download\n","from langchain.tools import tool\n","from langchain_community.chat_models import ChatLlamaCpp\n","from langchain_core.messages import HumanMessage"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z2VwSY6PazoH"},"outputs":[],"source":["# Download the LLM, you can search in Hugging Face\n","model_path = hf_hub_download(\n","    repo_id=\"Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF\",\n","    filename=\"qwen2.5-coder-0.5b-instruct-q4_k_m.gguf\",\n","    force_download=False,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lw-Bwwj6bYtG"},"outputs":[],"source":["# Create the LLM\n","llm = ChatLlamaCpp(\n","    model_path=model_path,\n","    n_gpu_layers=25,\n","    stop=[\"<|im_end|>\\n\"],\n","    n_ctx=4096,\n","    max_tokens=4096,\n","    streaming=False,\n","    n_batch=256,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"26TEU_ribafV"},"outputs":[],"source":["# Create tools\n","@tool\n","def get_curr_temperature(city: str) -> int:\n","    \"\"\"Get the current temperature of a city\"\"\"\n","    return randint(20, 30)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jBVknWU_bsPV"},"outputs":[],"source":["# Bind the tools to the LLM\n","llm_tools = llm.bind_tools(\n","  [get_curr_temperature],\n","  tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_curr_temperature\"}},\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vgE1LmIgb5us"},"outputs":[],"source":["# Invoke the LLM with tools\n","messages = [\n","  HumanMessage(\"What is the current temperature in Madrid?\")\n","]\n","\n","all_tools_res = llm_tools.invoke(messages)\n","\n","# We can append the LLM response to messages\n","# messages.append(all_tools_res)"]},{"cell_type":"code","source":["# Show the tool calls\n","all_tools_res.tool_calls"],"metadata":{"id":"aJHYHLesI3Q0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Call the tool\n","tool = all_tools_res.tool_calls[0]\n","tool_msg = get_curr_temperature.invoke(tool)"],"metadata":{"id":"Jrs3o5liJtZJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Add the tool response to the messages\n","tool_msg.additional_kwargs = {\"args\": tool[\"args\"]}\n","messages.append(tool_msg)"],"metadata":{"id":"bPMbTz2uKRUI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Invoke the LLM with the new messages\n","llm.invoke(messages)"],"metadata":{"id":"I4lS2eZ1KaJu"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}