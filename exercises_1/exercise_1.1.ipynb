{"cells":[{"cell_type":"markdown","metadata":{"id":"4c70109c"},"source":["# Exercise: Setting Up and Running a Llama-Cpp Model\n","\n","This notebook demonstrates how to install dependencies, download a model from Hugging Face,\n","and load it using the `llama_cpp` library. Follow the steps below to understand how to:\n","- Install necessary Python packages.\n","- Download a GGUF model file from Hugging Face.\n","- Load and interact with the model.\n","\n","Make sure you have the required dependencies installed before running the notebook."]},{"cell_type":"code","source":["# Check GPU availability\n","!nvidia-smi"],"metadata":{"id":"mg2NUeKavcBY"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eggKm2rsYYJk"},"outputs":[],"source":["# Install necessary dependencies for running the Llama model\n","!pip3 install llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124\n","!pip3 install huggingface_hub==0.28.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YLXagtryYd6g"},"outputs":[],"source":["# Import required libraries for model execution and downloading\n","from llama_cpp import Llama\n","from huggingface_hub import hf_hub_download"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MBKorQ47Yi3Q"},"outputs":[],"source":["# Download the LLM model file from Hugging Face\n","# Ensure the repository ID and filename match the desired model\n","# Download the LLM, you can search in Hugging Face for mode GGUF LLMs\n","model_path = hf_hub_download(\n","    repo_id=\"Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF\",\n","    filename=\"qwen2.5-coder-0.5b-instruct-q4_k_m.gguf\",\n","    force_download=False,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"84vQDd8FYjco"},"outputs":[],"source":["# Create the LLM\n","llm = Llama(\n","    model_path=model_path,\n","    # n_gpu_layers=-1, # Uncomment to use GPU acceleration\n","    n_ctx=2048,  # Uncomment to increase the context window\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2FatxTSiYk5v"},"outputs":[],"source":["# Prompt the LLM\n","llm(\n","    # prompt for the LLM with prefix and suffix\n","    (\n","        \"<|im_start|>user\\n\"\n","        \"Name the planets in the solar system<|im_end|>\\n\"\n","        \"<|im_start|>assistant\\n\"\n","    ),\n","    # Generate up to 2048 tokens, set to None to generate up to the end of the context window\n","    max_tokens=2048,\n","    # Stop generating just before the model would generate a new question\n","    stop=[\"<|im_end|>\\n\"],\n","    temperature=0.0\n",")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}