{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP1vHDZv8qeUXHMCFjpbofn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Exercise: Retrieval-Augmented Generation (RAG) with LangChain and Llama\n","\n","This notebook demonstrates how to build a simple RAG pipeline using `langchain`, `llama-cpp-python`, and `Chroma`.  \n","It follows these steps:\n","\n","1. **Install Dependencies**: Ensures all required packages are installed.\n","2. **Load Models**: Downloads an LLM and an embedding model from Hugging Face.\n","3. **Process Documents**: Fetches a blog post, extracts relevant content, splits it into chunks, and indexes it using a vector store.\n","4. **Set Up RAG Pipeline**: Defines a retriever to fetch relevant content and integrates it with a language model to generate responses.\n","5. **Test the RAG Pipeline**: Queries the model to retrieve and summarize information.\n","\n","Run the final cell to see how the system responds to a test query."],"metadata":{"id":"vxWV21f24Ced"}},{"cell_type":"code","source":["# Check GPU availability\n","!nvidia-smi"],"metadata":{"id":"Xjtm5qozvVDx"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2UcTgkNEahwC"},"outputs":[],"source":["# Install required dependencies\n","!pip3 install llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124\n","!pip3 install huggingface_hub==0.28.0\n","!pip3 install langchain==0.3.17 langchain-core==0.3.33 langchain-community==0.3.14\n","!pip3 install langchain-chroma==0.2.0 langchain_huggingface==0.1.2"]},{"cell_type":"code","source":["import bs4\n","\n","from huggingface_hub import hf_hub_download\n","from langchain_community.chat_models import ChatLlamaCpp\n","from langchain_huggingface import HuggingFaceEmbeddings\n","\n","from langchain_chroma import Chroma\n","from langchain_community.document_loaders import WebBaseLoader\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_core.messages import SystemMessage\n","from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n","from langchain_text_splitters import RecursiveCharacterTextSplitter"],"metadata":{"id":"XOrRc1SBavJg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Download the LLM, you can search in Hugging Face\n","model_path = hf_hub_download(\n","    repo_id=\"Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF\",\n","    filename=\"qwen2.5-coder-0.5b-instruct-q4_k_m.gguf\",\n","    force_download=False,\n",")"],"metadata":{"id":"Z2VwSY6PazoH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the LLM\n","llm = ChatLlamaCpp(\n","    model_path=model_path,\n","    n_gpu_layers=25,\n","    stop=[\"<|im_end|>\\n\"],\n","    n_ctx=8000,\n","    max_tokens=8000,\n","    streaming=True,\n","    n_batch=256,\n",")"],"metadata":{"id":"Lw-Bwwj6bYtG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the embedding\n","embedding = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")"],"metadata":{"id":"bMSposKxv5Ve"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load, chunk and index the contents of the blog\n","loader = WebBaseLoader(\n","    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n","    bs_kwargs=dict(\n","        parse_only=bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))\n","    ),\n",")\n","docs = loader.load()\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","splits = text_splitter.split_documents(docs)"],"metadata":{"id":"js9DUOe-tR29"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a vector store using Chroma and index the document chunks\n","vectorstore = Chroma.from_documents(documents=splits, embedding=embedding)\n","\n","# Create the retriever\n","retriever = vectorstore.as_retriever(search_kwargs={\"k\": 20})"],"metadata":{"id":"YDF5bulztVGn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the prompt\n","prompt = ChatPromptTemplate.from_messages(\n","    [\n","        SystemMessage(\"You are an AI assistant that answer questions briefly.\"),\n","        HumanMessagePromptTemplate.from_template(\n","            \"Taking into account the following information:{context}\\n\\n{question}\"\n","        ),\n","    ]\n","\n",")"],"metadata":{"id":"26TEU_ribafV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a format function\n","def format_docs(docs):\n","    formated_docs = \"\"\n","\n","    for d in docs:\n","        formated_docs += f\"\\n\\n\\t- {d.page_content}\"\n","\n","    return formated_docs\n","\n","\n","# Create the RAG chain\n","rag_chain = (\n","    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n","    | prompt\n","    | llm\n","    | StrOutputParser()\n",")"],"metadata":{"id":"jBVknWU_bsPV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Query the RAG system and stream the response\n","for c in rag_chain.stream(\"What is Task Decomposition?\"):\n","    print(c, flush=True, end=\"\")"],"metadata":{"id":"1duoOM5bcxJR"},"execution_count":null,"outputs":[]}]}