{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9f3491ff"
   },
   "source": [
    "# Exercise: Setting Up and Running LLaMA with LangChain\n",
    "\n",
    "This notebook guides you through setting up and running a LLaMA-based language model using `llama-cpp-python`, `huggingface_hub`, and `langchain`.\n",
    "It follows these steps:\n",
    "\n",
    "1. **Install Dependencies**: Ensures all required packages are installed.\n",
    "2. **Load Models**: Downloads an LLM model from Hugging Face.\n",
    "3. **Set Up the LLM Pipeline**: Setting up the language model and defining a simple chat interaction.\n",
    "\n",
    "Follow the instructions in the code cells and ensure all dependencies are installed correctly before proceeding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Hy4hPCzvgqh"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2UcTgkNEahwC"
   },
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "!pip3 install llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124\n",
    "!pip3 install huggingface_hub==0.28.0\n",
    "!pip3 install langchain==0.3.17 langchain-core==0.3.33 langchain-community==0.3.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XOrRc1SBavJg"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries for downloading models and setting up the chat system\n",
    "from huggingface_hub import hf_hub_download\n",
    "from langchain_community.chat_models import ChatLlamaCpp\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2VwSY6PazoH"
   },
   "outputs": [],
   "source": [
    "# Download the LLM, you can search in Hugging Face\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF\",\n",
    "    filename=\"qwen2.5-coder-0.5b-instruct-q4_k_m.gguf\",\n",
    "    force_download=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lw-Bwwj6bYtG"
   },
   "outputs": [],
   "source": [
    "# Create the LLM\n",
    "llm = ChatLlamaCpp(\n",
    "    model_path=model_path,\n",
    "    stop=[\"<|im_end|>\\n\"],\n",
    "    n_ctx=2048,\n",
    "    max_tokens=2048,\n",
    "    streaming=True,\n",
    "    n_batch=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "26TEU_ribafV"
   },
   "outputs": [],
   "source": [
    "# Create the prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\"You are an AI assistant that answer questions briefly.\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"Tell me a joke about {topic}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBVknWU_bsPV"
   },
   "outputs": [],
   "source": [
    "# Create the chain\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgE1LmIgb5us"
   },
   "outputs": [],
   "source": [
    "# Call the chain\n",
    "chain.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1duoOM5bcxJR"
   },
   "outputs": [],
   "source": [
    "# Now try the stream mode\n",
    "for s in chain.stream({\"topic\": \"ice cream\"}):\n",
    "  print(s, flush=True, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
