{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9f3491ff"
   },
   "source": [
    "# Exercise: SLERP Model Merging\n",
    "\n",
    "This notebook guides you through the creation of model by leveregin Model Mergin techniques using `Mergekit`.\n",
    "The steps include:\n",
    "\n",
    "1. Installing required dependencies.\n",
    "2. Importing necessary libraries.\n",
    "3. Setting up the merge configuration.\n",
    "4. Merging models.\n",
    "\n",
    "Follow the instructions in the code cells and ensure all dependencies are installed correctly before proceeding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Hy4hPCzvgqh"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2UcTgkNEahwC"
   },
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "!pip3 install mergekit==0.0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2VwSY6PazoH"
   },
   "outputs": [],
   "source": [
    "# Define the configuration for our Model Merging\n",
    "import yaml\n",
    "\n",
    "yaml_config = \"\"\"\n",
    "slices:\n",
    "  - sources:\n",
    "      - model: indischepartij/MiniCPM-3B-Hercules-v2.0\n",
    "        layer_range: [0, 40]\n",
    "      - model: indischepartij/MiniCPM-3B-Bacchus\n",
    "        layer_range: [0, 40]\n",
    "merge_method: slerp\n",
    "base_model: indischepartij/MiniCPM-3B-Hercules-v2.0\n",
    "parameters:\n",
    "  t:\n",
    "    - filter: self_attn\n",
    "      value: [0, 0.5, 0.3, 0.7, 1]\n",
    "    - filter: mlp\n",
    "      value: [1, 0.5, 0.7, 0.3, 0]\n",
    "    - value: 0.5\n",
    "dtype: bfloat16\n",
    "\"\"\"\n",
    "\n",
    "# Save config as yaml file\n",
    "with open(\"config.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(yaml_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lw-Bwwj6bYtG"
   },
   "outputs": [],
   "source": [
    "# Merge models\n",
    "# --copy-tokenizer to copy the tokenizer from the base model\n",
    "# --allow-crimes and --out-shard-size to chunk the models into smaller shards that can be computed on a CPU with low RAM\n",
    "# --lazy-unpickle to enable the experimental lazy unpickler for lower memory usage\n",
    "!mergekit-yaml config.yaml merge --copy-tokenizer --allow-crimes --out-shard-size 1B --lazy-unpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6AtGcW9dv0kY"
   },
   "outputs": [],
   "source": [
    "# Download and install llama.cpp\n",
    "!git clone https://github.com/ggerganov/llama.cpp -b b4710\n",
    "!cd llama.cpp && cmake -B build && cmake --build build --config Release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YayuClA5v1PX"
   },
   "outputs": [],
   "source": [
    "# Convert the model to llama.cpp\n",
    "!python3 llama.cpp/convert_hf_to_gguf.py merge\n",
    "!mv merge/DeepSeek-R1-Distill-Qwen-1.5B-F16.gguf merge/QwenSeek-1.5B-SLERP-f16.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aW6XqEGny22l"
   },
   "outputs": [],
   "source": [
    "# Quantize the model\n",
    "!llama.cpp/build/bin/llama-quantize merge/QwenSeek-1.5B-SLERP-f16.gguf merge/QwenSeek-1.5B-SLERP-Q4_K_M.gguf Q4_K_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your new model!!!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
