{"cells":[{"cell_type":"markdown","metadata":{"id":"9f3491ff"},"source":["# Exercise: Passthrough Model Merging\n","\n","This notebook guides you through the creation of model by leveregin Model Mergin techniques using `Mergekit`.\n","The steps include:\n","\n","1. Installing required dependencies.\n","2. Importing necessary libraries.\n","3. Setting up the merge configuration.\n","4. Merging models.\n","\n","Follow the instructions in the code cells and ensure all dependencies are installed correctly before proceeding.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Hy4hPCzvgqh"},"outputs":[],"source":["# Check GPU availability\n","!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2UcTgkNEahwC"},"outputs":[],"source":["# Install required dependencies\n","!pip3 install mergekit==0.0.6"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XOrRc1SBavJg"},"outputs":[],"source":["# Import necessary libraries for downloading models and setting up the chat system\n","import yaml"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z2VwSY6PazoH"},"outputs":[],"source":["# Define the configuration for our Model Merging\n","yaml_config = \"\"\"\n","slices:\n","- sources:\n","  - layer_range: [0, 12]\n","    model: Qwen/Qwen2.5-Coder-0.5B-Instruct\n","- sources:\n","  - layer_range: [6, 18]\n","    model: Qwen/Qwen2.5-Coder-0.5B-Instruct\n","- sources:\n","  - layer_range: [12, 24]\n","    model: Qwen/Qwen2.5-Coder-0.5B-Instruct\n","- sources:\n","  - layer_range: [24, 24]\n","    model: Qwen/Qwen2.5-Coder-0.5B-Instruct\n","merge_method: passthrough\n","dtype: bfloat16\n","\"\"\"\n","\n","# Save config as yaml file\n","with open(\"config.yaml\", \"w\", encoding=\"utf-8\") as f:\n","    f.write(yaml_config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lw-Bwwj6bYtG"},"outputs":[],"source":["# Merge models\n","# --copy-tokenizer to copy the tokenizer from the base model\n","# --allow-crimes and --out-shard-size to chunk the models into smaller shards that can be computed on a CPU with low RAM\n","# --lazy-unpickle to enable the experimental lazy unpickler for lower memory usage\n","!mergekit-yaml config.yaml merge --copy-tokenizer --allow-crimes --out-shard-size 1B --lazy-unpickle"]},{"cell_type":"code","source":["# Download and install llama.cpp\n","!git clone https://github.com/ggerganov/llama.cpp -b b4710\n","!cd llama.cpp && cmake -B build && cmake --build build --config Release"],"metadata":{"id":"6AtGcW9dv0kY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert the model to llama.cpp\n","!python3 llama.cpp/convert_hf_to_gguf.py merge"],"metadata":{"id":"YayuClA5v1PX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Quantize the model\n","!llama.cpp/build/bin/llama-quantize merge/Qwen2.5-Coder-0.5B-Instruct-F16.gguf merge/Qwen2.5-Coder-0.5B-Instruct-Q4_K_M.gguf Q4_K_M"],"metadata":{"id":"aW6XqEGny22l"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}