{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "```shell\n",
    "$ cd ~/ros2_ws/src\n",
    "$ git clone --recurse-submodules https://github.com/mgonzs13/llama_ros.git\n",
    "$ pip3 install -r llama_ros/requirements.txt\n",
    "$ cd ~/ros2_ws\n",
    "$ colcon build\n",
    "```\n",
    "\n",
    "### CUDA\n",
    "\n",
    "To run llama_ros with CUDA, first, you must install the [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit). Then, you have to set the environment variable `LLAMA_CUDA` to `on`:\n",
    "\n",
    "```shell\n",
    "export LLAMA_CUDA=\"on\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### llama_cli\n",
    "\n",
    "Commands are included in llama_ros to speed up the test of GGUF-based LLMs within the ROS 2 ecosystem. This way, the following commands are integrating into the ROS 2 commands:\n",
    "\n",
    "#### launch\n",
    "\n",
    "Using this command launch a LLM from a YAML file. The configuration of the YAML is used to launch the LLM in the same way as using a regular launch file. Here is an example of how to use it:\n",
    "\n",
    "```shell\n",
    "$ ros2 llama launch ~/ros2_ws/src/llama_ros/llama_bringup/params/StableLM-Zephyr.yaml\n",
    "```\n",
    "\n",
    "#### prompt\n",
    "\n",
    "Using this command send a prompt to a launched LLM. The command uses a string, which is the prompt; and the temperature value. Here is an example of how to use it:\n",
    "\n",
    "```shell\n",
    "$ ros2 llama prompt \"Do you know ROS 2?\" -t 0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Files\n",
    "\n",
    "First of all, you need to create a launch file to use llama_ros or llava_ros. This launch file will contain the main parameters to download the model from HuggingFace and configure it. Take a look at the following examples and the [predefined launch files](https://github.com/mgonzs13/llama_ros/tree/main/llama_bringup/launch).\n",
    "\n",
    "#### llama_ros (Python Launch)\n",
    "\n",
    "<details>\n",
    "<summary>Click to expand</summary>\n",
    "\n",
    "```python\n",
    "from launch import LaunchDescription\n",
    "from llama_bringup.utils import create_llama_launch\n",
    "\n",
    "\n",
    "def generate_launch_description():\n",
    "\n",
    "    return LaunchDescription([\n",
    "        create_llama_launch(\n",
    "            n_ctx=2048, # context of the LLM in tokens\n",
    "            n_batch=8, # batch size in tokens\n",
    "            n_gpu_layers=0, # layers to load in GPU\n",
    "            n_threads=1, # threads\n",
    "            n_predict=2048, # max tokens, -1 == inf\n",
    "\n",
    "            model_repo=\"TheBloke/Marcoroni-7B-v3-GGUF\", # Hugging Face repo\n",
    "            model_filename=\"marcoroni-7b-v3.Q4_K_M.gguf\", # model file in repo\n",
    "\n",
    "            system_prompt_type=\"alpaca\" # system prompt type\n",
    "        )\n",
    "    ])\n",
    "```\n",
    "\n",
    "```shell\n",
    "$ ros2 launch llama_bringup marcoroni.launch.py\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "#### llama_ros (YAML Config)\n",
    "\n",
    "<details>\n",
    "<summary>Click to expand</summary>\n",
    "\n",
    "```yaml\n",
    "n_ctx: 2048 # context of the LLM in tokens\n",
    "n_batch: 8 # batch size in tokens\n",
    "n_gpu_layers: 0 # layers to load in GPU\n",
    "n_threads: 1 # threads\n",
    "n_predict: 2048 # max tokens, -1 == inf\n",
    "\n",
    "model_repo: \"cstr/Spaetzle-v60-7b-GGUF\" # Hugging Face repo\n",
    "model_filename: \"Spaetzle-v60-7b-q4-k-m.gguf\" # model file in repo\n",
    "\n",
    "system_prompt_type: \"Alpaca\" # system prompt type\n",
    "```\n",
    "\n",
    "```python\n",
    "import os\n",
    "from launch import LaunchDescription\n",
    "from llama_bringup.utils import create_llama_launch_from_yaml\n",
    "from ament_index_python.packages import get_package_share_directory\n",
    "\n",
    "\n",
    "def generate_launch_description():\n",
    "    return LaunchDescription([\n",
    "        create_llama_launch_from_yaml(os.path.join(\n",
    "            get_package_share_directory(\"llama_bringup\"), \"params\", \"Spaetzle.yaml\"))\n",
    "    ])\n",
    "```\n",
    "\n",
    "```shell\n",
    "$ ros2 launch llama_bringup spaetzle.launch.py\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "#### llava_ros (Python Launch)\n",
    "\n",
    "<details>\n",
    "<summary>Click to expand</summary>\n",
    "\n",
    "```python\n",
    "from launch import LaunchDescription\n",
    "from llama_bringup.utils import create_llama_launch\n",
    "\n",
    "def generate_launch_description():\n",
    "\n",
    "    return LaunchDescription([\n",
    "        create_llama_launch(\n",
    "            use_llava=True, # enable llava\n",
    "            embedding=False, # disable embeddings\n",
    "\n",
    "            n_ctx=8192, # context of the LLM in tokens, use a huge context size to load images\n",
    "            n_batch=512, # batch size in tokens\n",
    "            n_gpu_layers=33, # layers to load in GPU\n",
    "            n_threads=1, # threads\n",
    "            n_predict=8192, # max tokens, -1 == inf\n",
    "\n",
    "            model_repo=\"cjpais/llava-1.6-mistral-7b-gguf\", # Hugging Face repo\n",
    "            model_filename=\"llava-v1.6-mistral-7b.Q4_K_M.gguf\", # model file in repo\n",
    "\n",
    "            mmproj_repo=\"cjpais/llava-1.6-mistral-7b-gguf\", # Hugging Face repo\n",
    "            mmproj_filename=\"mmproj-model-f16.gguf\", # mmproj file in repo\n",
    "\n",
    "            system_prompt_type=\"mistral\" # system prompt type\n",
    "        )\n",
    "    ])\n",
    "```\n",
    "\n",
    "```shell\n",
    "$ ros2 launch llama_bringup llava.launch.py\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "#### llava_ros (YAML Config)\n",
    "\n",
    "<details>\n",
    "<summary>Click to expand</summary>\n",
    "\n",
    "```yaml\n",
    "use_llava: True # enable llava\n",
    "embedding: False # disable embeddings\n",
    "\n",
    "n_ctx: 8192 # context of the LLM in tokens use a huge context size to load images\n",
    "n_batch: 512 # batch size in tokens\n",
    "n_gpu_layers: 33 # layers to load in GPU\n",
    "n_threads: 1 # threads\n",
    "n_predict: 8192 # max tokens -1 : :  inf\n",
    "\n",
    "model_repo: \"cjpais/llava-1.6-mistral-7b-gguf\" # Hugging Face repo\n",
    "model_filename: \"llava-v1.6-mistral-7b.Q4_K_M.gguf\" # model file in repo\n",
    "\n",
    "mmproj_repo: \"cjpais/llava-1.6-mistral-7b-gguf\" # Hugging Face repo\n",
    "mmproj_filename: \"mmproj-model-f16.gguf\" # mmproj file in repo\n",
    "\n",
    "system_prompt_type: \"mistral\" # system prompt type\n",
    "```\n",
    "\n",
    "```python\n",
    "def generate_launch_description():\n",
    "    return LaunchDescription([\n",
    "        create_llama_launch_from_yaml(os.path.join(\n",
    "            get_package_share_directory(\"llama_bringup\"),\n",
    "            \"params\", \"llava-1.6-mistral-7b-gguf.yaml\"))\n",
    "    ])\n",
    "```\n",
    "\n",
    "```shell\n",
    "$ ros2 launch llama_bringup llava.launch.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROS 2 Clients\n",
    "\n",
    "Both llama_ros and llava_ros provide ROS 2 interfaces to access the main functionalities of the models. Here you have some examples of how to use them inside ROS 2 nodes. Moreover, take a look to the [llama_client_node.py](https://github.com/mgonzs13/llama_ros/blob/main/llama_ros/llama_ros/llama_client_node.py) and [llava_client_node.py](https://github.com/mgonzs13/llama_ros/blob/main/llama_ros/llama_ros/llava_client_node.py) examples.\n",
    "\n",
    "#### Tokenize\n",
    "\n",
    "<details>\n",
    "<summary>Click to expand</summary>\n",
    "\n",
    "```python\n",
    "from rclpy.node import Node\n",
    "from llama_msgs.srv import Tokenize\n",
    "\n",
    "\n",
    "class ExampleNode(Node):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\"example_node\")\n",
    "\n",
    "        # create the client\n",
    "        self.srv_client = self.create_client(Tokenize, \"/llama/tokenize\")\n",
    "\n",
    "        # create the request\n",
    "        req = Tokenize.Request()\n",
    "        req.prompt = \"Example text\"\n",
    "\n",
    "        # call the tokenize service\n",
    "        self.srv_client.wait_for_service()\n",
    "        res = self.srv_client.call(req)\n",
    "        tokens = res.tokens\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Embeddings\n",
    "\n",
    "<details>\n",
    "<summary>Click to expand</summary>\n",
    "\n",
    "```python\n",
    "from rclpy.node import Node\n",
    "from llama_msgs.srv import Embeddings\n",
    "\n",
    "\n",
    "class ExampleNode(Node):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\"example_node\")\n",
    "\n",
    "        # create the client\n",
    "        self.srv_client = self.create_client(Embeddings, \"/llama/generate_embeddings\")\n",
    "\n",
    "        # create the request\n",
    "        req = Embeddings.Request()\n",
    "        req.prompt = \"Example text\"\n",
    "        req.normalize = True\n",
    "\n",
    "        # call the embedding service\n",
    "        self.srv_client.wait_for_service()\n",
    "        res = self.srv_client.call(req)\n",
    "        embeddings = res.embeddings\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Generate Response\n",
    "\n",
    "<details>\n",
    "<summary>Click to expand</summary>\n",
    "\n",
    "```python\n",
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "from rclpy.action import ActionClient\n",
    "from llama_msgs.action import GenerateResponse\n",
    "\n",
    "\n",
    "class ExampleNode(Node):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\"example_node\")\n",
    "\n",
    "        # create the client\n",
    "        self.action_client = ActionClient(\n",
    "            self, GenerateResponse, \"/llama/generate_response\")\n",
    "\n",
    "        # create the goal and set the sampling config\n",
    "        goal = GenerateResponse.Goal()\n",
    "        goal.prompt = self.prompt\n",
    "        goal.sampling_config.temp = 0.2\n",
    "\n",
    "        # wait for the server and send the goal\n",
    "        self.action_client.wait_for_server()\n",
    "        send_goal_future = self.action_client.send_goal_async(\n",
    "            goal)\n",
    "\n",
    "        # wait for the server\n",
    "        rclpy.spin_until_future_complete(self, send_goal_future)\n",
    "        get_result_future = send_goal_future.result().get_result_async()\n",
    "\n",
    "        # wait again and take the result\n",
    "        rclpy.spin_until_future_complete(self, get_result_future)\n",
    "        result: GenerateResponse.Result = get_result_future.result().result\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Generate Response (llava)\n",
    "\n",
    "<details>\n",
    "<summary>Click to expand</summary>\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "from cv_bridge import CvBridge\n",
    "\n",
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "from rclpy.action import ActionClient\n",
    "from llama_msgs.action import GenerateResponse\n",
    "\n",
    "\n",
    "class ExampleNode(Node):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\"example_node\")\n",
    "\n",
    "        # create a cv bridge for the image\n",
    "        self.cv_bridge = CvBridge()\n",
    "\n",
    "        # create the client\n",
    "        self.action_client = ActionClient(\n",
    "            self, GenerateResponse, \"/llama/generate_response\")\n",
    "\n",
    "        # create the goal and set the sampling config\n",
    "        goal = GenerateResponse.Goal()\n",
    "        goal.prompt = self.prompt\n",
    "        goal.sampling_config.temp = 0.2\n",
    "\n",
    "        # add your image to the goal\n",
    "        image = cv2.imread(\"/path/to/your/image\", cv2.IMREAD_COLOR)\n",
    "        goal.image = self.cv_bridge.cv2_to_imgmsg(image)\n",
    "\n",
    "        # wait for the server and send the goal\n",
    "        self.action_client.wait_for_server()\n",
    "        send_goal_future = self.action_client.send_goal_async(\n",
    "            goal)\n",
    "\n",
    "        # wait for the server\n",
    "        rclpy.spin_until_future_complete(self, send_goal_future)\n",
    "        get_result_future = send_goal_future.result().get_result_async()\n",
    "\n",
    "        # wait again and take the result\n",
    "        rclpy.spin_until_future_complete(self, get_result_future)\n",
    "        result: GenerateResponse.Result = get_result_future.result().result\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain\n",
    "\n",
    "There is a [llama_ros integration for LangChain](https://github.com/mgonzs13/llama_ros/tree/main/llama_ros/llama_ros/langchain). Thus, prompt engineering techniques could be applied. Here you have an example to use it.\n",
    "\n",
    "#### llama_ros (Chain)\n",
    "\n",
    "<details>\n",
    "<summary>Click to expand</summary>\n",
    "\n",
    "```python\n",
    "import rclpy\n",
    "from llama_ros.langchain import LlamaROS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "rclpy.init()\n",
    "\n",
    "# create the llama_ros llm for langchain\n",
    "llm = LlamaROS()\n",
    "\n",
    "# create a prompt template\n",
    "prompt_template = \"tell me a joke about {topic}\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=prompt_template\n",
    ")\n",
    "\n",
    "# create a chain with the llm and the prompt template\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "# run the chain\n",
    "text = llm_chain.invoke({\"topic\": \"bears\"})\n",
    "\n",
    "rclpy.shutdown()\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "#### llama_ros_embeddings (RAG)\n",
    "\n",
    "<details>\n",
    "<summary>Click to expand</summary>\n",
    "\n",
    "```python\n",
    "import rclpy\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from llama_ros.langchain import LlamaROSEmbeddings\n",
    "\n",
    "\n",
    "rclpy.init()\n",
    "\n",
    "# create the llama_ros embeddings for lanchain\n",
    "embeddings = LlamaROSEmbeddings()\n",
    "\n",
    "# create a vector database and assign it\n",
    "db = Chroma(embedding_function=embeddings)\n",
    "\n",
    "# create the retriever\n",
    "retriever = self.db.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# add your texts\n",
    "db.add_texts(texts=[\"your_texts\"])\n",
    "\n",
    "# retrieve documents\n",
    "docuemnts = self.retriever.get_relevant_documents(\"your_query\")\n",
    "\n",
    "rclpy.shutdown()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
